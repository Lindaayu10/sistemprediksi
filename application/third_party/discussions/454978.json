[
  {
    "Id": "1086544",
    "ThreadId": "454978",
    "Html": "Hi there!<br />\n<br />\nI'm trying to extract some information from a large Excel file, with something like 40852 rows by 38 columns. <br />\n<br />\nTaking into consideration the large amount of data, I'm already several recommended techniques for this situations, like caching and read by chunks. However, even if I define a chunk size of 1, the memory still runs out, probably during the parsing process.<br />\n<br />\nHere are my questions:<br />\n<ol>\n<li>How can I be sure that the memory exhaustion is occurring during the parsing process?</li>\n<li>\nIf so, what can I do to prevent this situation?<br />\n</li>\n</ol>\nThanks in advance for all the help,<br />\nBest regards!<br />\n",
    "PostedDate": "2013-08-30T02:27:52.043-07:00",
    "UserRole": null,
    "MarkedAsAnswerDate": null
  },
  {
    "Id": "1086742",
    "ThreadId": "454978",
    "Html": "As you read each line open a file and append it as csv data and then close the file. <br />\n<br />\nWhen you get the out of memory error, check to see if the data stopped in the middle.<br />\n<br />\nThat will at least let you know if you are running out of memory during parsing.<br />\n<br />\nNote: Reading in a CSV file requires less memory than a spreadsheet.<br />\n<ul>\n<li>Christopher Mullins</li>\n</ul>\n",
    "PostedDate": "2013-08-30T10:19:15.217-07:00",
    "UserRole": null,
    "MarkedAsAnswerDate": null
  }
]