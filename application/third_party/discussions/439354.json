[
  {
    "Id": "1026549",
    "ThreadId": "439354",
    "Html": "PHPExcel 1.7.8\r<br />\nPHP 5.3\r<br />\nI've got a 2012 MBA with 4gb ram. 512M as php memory limit.\r<br />\n<br />\nI'm trying to read a big excel file of about 20mb to import into mysql.\r<br />\n<br />\nI've searched across internet and found the &quot;Chunks reading&quot; solution, I checked the examples (11 and 12) and tried them with others solutions on the internet. However is not working... or is SO slowly for me, and I'm not sure why.\r<br />\n<br />\nThis is what im doing:<br />\n<pre><code>// .....\n// into MyReadFilter class.. this is the most important function, you already know it because is almost the same on all versions of filters to read in chunks\npublic function readCell($column, $row, $worksheetName = '') {\n        //  Only read the rows and columns that were configured\n        if (($row == 1) || ($row &gt;= $this-&gt;_startRow &amp;&amp; $row &lt; $this-&gt;_endRow)) {\n            if (in_array($column,$this-&gt;_columns)) {\n                return true;\n            }\n        }\n        return false;\n    }\n// .....\n\n\n// now my code...\n$filter = new MyReadFilter(1, 22000); \n$chunkSize = 10;\n\n$objReader = PHPExcel_IOFactory::createReader($inputFileType);\n$objReader-&gt;setReadFilter($filter);\n$objReader-&gt;setReadDataOnly(false); //not sure if this should be true\n\nfor ($startRow = 2; $startRow &lt;= 65536; $startRow += $chunkSize) {\n\n  echo &quot;Reading&quot;;\n  $filterSubset-&gt;setRows($startRow, $chunkSize);\n  $objPHPExcel = $objReader-&gt;load($inputFileName); // this line takes like 40 seconds... for 10 rows?\n  echo &quot;chunk done! &quot;;\n}\n\n</code></pre>\n\nHowever, inside the <strong>for</strong>, the <strong>$objReader-&gt;load()</strong> is taking like <strong>40 seconds</strong>, and in fact, after 2 loops I still get a memory error. \r<br />\n<br />\nIf I unset() the $objReader <strong>inside the for</strong> I can make the loop run about 20 times... (although it take like 10 minutes) and.. memory error.\r<br />\n<br />\nI'm wondering why the load function seems to read all the file if im using a filter, also the filter strategy seems to parse all rows and return false for all rows that are not being required... so from my point of view is not efficient as i have like 100k rows... Is not posible to just abort reading or really read just the required rows?\r<br />\n<br />\nI've tried a couple of FilterClass and code snippets but got same results...\r<br />\n<br />\nSo... suggestions?<br />\n",
    "PostedDate": "2013-04-06T07:12:17.717-07:00",
    "UserRole": null,
    "MarkedAsAnswerDate": null
  }
]