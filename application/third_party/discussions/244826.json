[
  {
    "Id": "561394",
    "ThreadId": "244826",
    "Html": "\r\n<p>I have been going through a number of these forum entries / discussions, and I'm still a bit confused. &nbsp;I ran a few benchmarks myself to get an idea of the software.</p>\r\n<p>What I am trying to do is mass import (100-100,000) rows or XLS (limit of course of 56k) or XLSX. I would need to read the first row (column names) in order to analyse the data columns needed.<br>\r\nFollowing that, I would take only the columns that I need (this data import might have other columns that are not needed.. this is for the purpose of say purchase order information).</p>\r\n<p>The required columns would then be imported into my database (via&nbsp;iteration&nbsp;of some sort), currently I have tried the following code:</p>\r\n<p></p>\r\n<div style=\"color:black; background-color:white\">\r\n<pre>$inputFileName = <span style=\"color:#a31515\">&quot;/var/www/default/uploads/&quot;</span>.$<span style=\"color:blue\">file</span>;\r\n\r\n<span style=\"color:green\">/**  Identify the type of $inputFileName  **/</span>\r\n$inputFileType = PHPExcel_IOFactory::identify($inputFileName);\r\n\r\n<span style=\"color:green\">/**  Create a new Reader of the type defined in $inputFileType  **/</span> \r\n$reader = PHPExcel_IOFactory::createReader($inputFileType); \r\n\r\n<span style=\"color:green\">/**  Define how many rows we want to read for each &quot;chunk&quot;  **/</span> \r\n$chunkSize =2048; \r\n<span style=\"color:green\">/**  Create a new Instance of our Read Filter  **/</span> \r\n$chunkFilter = <span style=\"color:blue\">new</span> chunkReadFilter(); \r\n\r\n<span style=\"color:green\">/**  Tell the Reader that we want to use the Read Filter  **/</span> \r\n$reader-&gt;setReadFilter($chunkFilter); \r\n$reader-&gt;setReadDataOnly(True);\r\n\r\n<span style=\"color:blue\">echo</span> <span style=\"color:#a31515\">&quot;&lt;pre&gt;&quot;</span>;\r\n\r\n<span style=\"color:green\">/**  Loop to read our worksheet in &quot;chunk size&quot; blocks  **/</span> \r\n<span style=\"color:blue\">for</span> ($startRow = 2; $startRow &lt;= 65536; $startRow &#43;= $chunkSize) { \r\n\t<span style=\"color:green\">/**  Tell the Read Filter which rows we want this iteration  **/</span> \r\n\t$chunkFilter-&gt;setRows($startRow,$chunkSize); \r\n\t<span style=\"color:green\">/**  Load only the rows that match our filter  **/</span> \r\n\t$objPHPExcel = $reader-&gt;load($inputFileName); \r\n\r\n\t<span style=\"color:green\">//    Do some processing here </span>\r\n\tprint_r( $objPHPExcel-&gt;getActiveSheet()-&gt;toArray() );\r\n\r\n\t<span style=\"color:blue\">if</span>(count( $objPHPExcel-&gt;getActiveSheet()-&gt;toArray() ) &lt; $chunkSize){\r\n\t\t$startRow = 65536;\r\n\t}\r\n\t\r\n\t<span style=\"color:green\">//    Free up some of the memory </span>\r\n\t$objPHPExcel-&gt;disconnectWorksheets(); \r\n\t<span style=\"color:blue\">unset</span>($objPHPExcel); \t\t\t\t\r\n} \t\t\t\r\n\r\n<span style=\"color:blue\">echo</span> <span style=\"color:#a31515\">&quot;&lt;/pre&gt;&quot;</span>;\t\t\t\t\r\n</pre>\r\n</div>\r\n<p></p>\r\n<p>My processing would most likely involve me creating a SQL insert query and inserting every 100 records or so, etc;.</p>\r\n<p>I am finding that no matter what I have done chunk size, large files usually time out (300&#43; sec) or run out of memory even tho I unset the $objPHPExcel object.<br>\r\nAs you proably can figure out by my copy &amp; paste code, I am very new to PHPExcel.&nbsp;</p>\r\n",
    "PostedDate": "2011-02-05T21:22:01.987-08:00",
    "UserRole": null,
    "MarkedAsAnswerDate": null
  },
  {
    "Id": "561429",
    "ThreadId": "244826",
    "Html": "\r\n<p>Assuming you're using the chunkReadFilter() that I posted here a few weeks ago, you're still reading every worksheet, and the chunkReadFilter() will read the same chunk from each worksheet in the workbook. Set your chunk reader to read only from one worksheet\r\n at a time, or use $objReader-&gt;setLoadSheetsOnly($sheetname) to force the reader only to work with a single, names worksheet at a time;</p>\r\n<p>I've added a listWorksheetNames() method to the Readers for the next release that would allow you to get a list of all the sheet names without reading in the rest of the file. You could call that before your main loop, and then chunk through each of those\r\n worksheets in turn.</p>\r\n<p>Be careful using $reader-&gt;setReadDataOnly(True) as you may have problems identifying dates in the worksheet when you want to actually process the cell data... you won't be able to differentiate between a date and a number.</p>\r\n<p>&nbsp;</p>\r\n<p>$objPHPExcel-&gt;getActiveSheet()-&gt;toArray() is a slow and memory-expensive test to identify the data that has been loaded.&nbsp;Again, with the latest SVN code, I've reduced the memory overhead of toArray(), and made it faster, and I've added an additional\r\n rangeToArray() method that allows you to specify a range of the worksheet to return as an array, e.g. 'A'.$startRow.':Z'.($startRow&#43;$chunkSize)</p>\r\n<p>&nbsp;</p>\r\n<p>Chunking is a lot slower than reading the entire workbook just once, because it has to parse the file every iteration of the loop. It's basically a &quot;trade off&quot; between speed and memory usage. It isn't designed for &quot;online&quot; work, but for background processing\r\n where you can increase the timeout value. 300 seconds is likely to be the Web Server timeout.</p>\r\n",
    "PostedDate": "2011-02-06T03:49:46.38-08:00",
    "UserRole": null,
    "MarkedAsAnswerDate": null
  }
]